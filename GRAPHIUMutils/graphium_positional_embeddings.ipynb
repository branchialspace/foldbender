{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTsVknoHbUSV"
      },
      "outputs": [],
      "source": [
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html > /dev/null 2>&1\n",
        "!pip install torch-geometric > /dev/null 2>&1\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/torch-2.0.0+cu118.html > /dev/null 2>&1\n",
        "!pip install git+https://github.com/datamol-io/graphium.git@2.2.0 > /dev/null 2>&1\n",
        "!pip install rdkit > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import storage\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/content/drive/MyDrive/instant-tape-******.json\"\n",
        "storage_client = storage.Client()"
      ],
      "metadata": {
        "id": "8p8Cz9lbbbUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Walk Structural Encoding\n",
        "from typing import Tuple, Union, Optional, List, Dict, Any, Iterable\n",
        "import torch\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def compute_rwse(\n",
        "    adj: torch.Tensor,\n",
        "    ksteps: Union[int, List[int]],\n",
        "    num_nodes: int,\n",
        "    cache: Dict[str, Any],\n",
        "    pos_type: str = \"rw_return_probs\",\n",
        "    space_dim: int = 0,\n",
        ") -> Tuple[torch.Tensor, str, Dict[str, Any]]:\n",
        "\n",
        "    base_level = \"node\" if pos_type == \"rw_return_probs\" else \"nodepair\"\n",
        "\n",
        "    # Manually handles edge case of 1 atom molecules here\n",
        "    if not isinstance(ksteps, Iterable):\n",
        "        ksteps = list(range(1, ksteps + 1))\n",
        "    if num_nodes == 1:\n",
        "        if pos_type == \"rw_return_probs\":\n",
        "            return torch.ones((1, len(ksteps))), base_level, cache\n",
        "        else:\n",
        "            return torch.ones((1, 1, len(ksteps))), base_level, cache\n",
        "\n",
        "    edge_index, edge_weight = adj.nonzero(as_tuple=True), adj[adj != 0]\n",
        "\n",
        "    # Compute the random-walk transition probabilities\n",
        "    if \"ksteps\" in cache:\n",
        "        cached_k = cache[\"ksteps\"]\n",
        "        missing_k = [k for k in ksteps if k not in cached_k]\n",
        "        if not missing_k:\n",
        "            pass\n",
        "        elif min(missing_k) < min(cached_k):\n",
        "            Pk_dict = get_Pks(missing_k, edge_index=edge_index, edge_weight=edge_weight, num_nodes=num_nodes)\n",
        "            cache[\"ksteps\"] = sorted(missing_k + cached_k)\n",
        "            for k in missing_k:\n",
        "                cache[\"Pk\"][k] = Pk_dict[k]\n",
        "        else:\n",
        "            start_k = min([max(cached_k), min(missing_k)])\n",
        "            start_Pk = cache[\"Pk\"][start_k]\n",
        "            Pk_dict = get_Pks(\n",
        "                missing_k,\n",
        "                edge_index=edge_index,\n",
        "                edge_weight=edge_weight,\n",
        "                num_nodes=num_nodes,\n",
        "                start_Pk=start_Pk,\n",
        "                start_k=start_k,\n",
        "            )\n",
        "            cache[\"ksteps\"] = sorted(cached_k + missing_k)\n",
        "            for k in missing_k:\n",
        "                cache[\"Pk\"][k] = Pk_dict[k]\n",
        "    else:\n",
        "        Pk_dict = get_Pks(ksteps, edge_index=edge_index, edge_weight=edge_weight, num_nodes=num_nodes)\n",
        "        cache[\"ksteps\"] = list(Pk_dict.keys())\n",
        "        cache[\"Pk\"] = Pk_dict\n",
        "\n",
        "    pe_list = []\n",
        "    if pos_type == \"rw_return_probs\":\n",
        "        for k in ksteps:\n",
        "            pe_list.append(torch.diagonal(cache[\"Pk\"][k]) * (k ** (space_dim / 2)))\n",
        "    else:\n",
        "        for k in ksteps:\n",
        "            pe_list.append(cache[\"Pk\"][k])\n",
        "\n",
        "    pe = torch.stack(pe_list, dim=-1)\n",
        "    return pe, base_level, cache\n",
        "\n",
        "def get_Pks(\n",
        "    ksteps: List[int],\n",
        "    edge_index: Tuple[torch.Tensor, torch.Tensor],\n",
        "    edge_weight: Optional[torch.Tensor] = None,\n",
        "    num_nodes: Optional[int] = None,\n",
        "    start_Pk: Optional[torch.Tensor] = None,\n",
        "    start_k: Optional[int] = None,\n",
        ") -> Dict[int, torch.Tensor]:\n",
        "\n",
        "    edge_index = (edge_index[0].to(device, dtype=torch.int64), edge_index[1].to(device, dtype=torch.int64))\n",
        "    batch = torch.zeros(num_nodes, dtype=torch.long).to(device)\n",
        "\n",
        "    if edge_weight is not None:\n",
        "        edge_weight = edge_weight.to(device).float()\n",
        "    if start_Pk is not None:\n",
        "        start_Pk = start_Pk.to(device).float()\n",
        "\n",
        "    if edge_weight is None:\n",
        "        edge_weight = torch.ones(edge_index[0].size(0), device=edge_index[0].device, dtype=torch.float32)\n",
        "\n",
        "    num_nodes = int(maybe_num_nodes(edge_index, num_nodes))\n",
        "    src = edge_index[0]\n",
        "    deg = scatter_add(edge_weight, src, dim=0, dim_size=num_nodes)  # Out degrees\n",
        "    deg_inv = deg.pow(-1.0)\n",
        "    deg_inv.masked_fill_(deg_inv == float(\"inf\"), 0)\n",
        "\n",
        "    if edge_index[0].numel() == 0:\n",
        "        P = edge_index[0].new_zeros((num_nodes, num_nodes))\n",
        "    else:\n",
        "        P = torch.diag(deg_inv).float() @ to_dense_adj(\n",
        "            edge_index, edge_attr=edge_weight, batch=batch, max_num_nodes=num_nodes\n",
        "        ).squeeze(0)\n",
        "\n",
        "        # P = torch.diag(deg_inv).float() @ to_dense_adj(\n",
        "        #    edge_index, edge_weight, max_num_nodes=num_nodes\n",
        "        #).squeeze(0)\n",
        "\n",
        "    if start_Pk is not None:\n",
        "        Pk = start_Pk @ P.clone().detach().matrix_power(min(ksteps) - start_k)\n",
        "    else:\n",
        "        Pk = P.clone().detach().matrix_power(min(ksteps))\n",
        "\n",
        "    Pk_dict = {}\n",
        "    for k in range(min(ksteps), max(ksteps) + 1):\n",
        "        Pk_dict[k] = Pk\n",
        "        Pk = Pk @ P\n",
        "\n",
        "    return Pk_dict"
      ],
      "metadata": {
        "id": "OEqibdC9i37a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplacian Positional Encoding\n",
        "import torch\n",
        "from typing import Tuple, Union, Dict, Any\n",
        "\n",
        "def compute_laplacian_pe(\n",
        "    adj: torch.Tensor,\n",
        "    num_pos: int,\n",
        "    cache: Dict[str, Any],\n",
        "    normalization: str = \"none\",\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, str, Dict[str, Any]]:\n",
        "\n",
        "    base_level = \"node\"\n",
        "    device = adj.device\n",
        "\n",
        "    # Convert dense tensor to sparse tensor if not already sparse\n",
        "    if not adj.is_sparse:\n",
        "        if \"csr_adj\" not in cache:\n",
        "            adj = adj.to_sparse()\n",
        "            cache[\"csr_adj\"] = adj\n",
        "        else:\n",
        "            adj = cache[\"csr_adj\"]\n",
        "\n",
        "    # Compute the Laplacian, and normalize it\n",
        "    if f\"L_{normalization}_sp\" not in cache:\n",
        "        D = torch.sum(adj, dim=1).flatten()\n",
        "\n",
        "        # Create a diagonal matrix D_mat directly on GPU\n",
        "        n = D.size(0)\n",
        "        eye = torch.eye(n, device=D.device)\n",
        "        D_mat = eye * D.unsqueeze(0)\n",
        "\n",
        "        L = -adj + D_mat\n",
        "        L_norm = normalize_matrix(L, degree_vector=D, normalization=normalization)\n",
        "        cache[f\"L_{normalization}_sp\"] = L_norm\n",
        "    else:\n",
        "        L_norm = cache[f\"L_{normalization}_sp\"]\n",
        "\n",
        "    # Compute the eigenvectors for the graph\n",
        "    if \"lap_eig\" not in cache:\n",
        "        epsilon = 1e-8\n",
        "        L_norm = L_norm.to_dense()  # Convert L_norm to a dense tensor\n",
        "        L_norm += torch.eye(L_norm.size(0), device=L_norm.device) * epsilon\n",
        "\n",
        "        # Convert back to sparse if needed (though for eigenvalue computation, dense might be better)\n",
        "        L_norm = L_norm.to_sparse()\n",
        "\n",
        "        eigvals, eigvecs = _get_positional_eigvecs(L_norm, num_pos=num_pos)\n",
        "\n",
        "        eigvecs[~torch.isfinite(eigvecs)] = 0.0\n",
        "        eigvals[~torch.isfinite(eigvals)] = 0.0\n",
        "\n",
        "        # repeat eigenvals for each node\n",
        "        eigvals = eigvals.unsqueeze(0).repeat(adj.shape[0], 1)\n",
        "\n",
        "        cache[\"lap_eig\"] = (eigvals, eigvecs)\n",
        "    else:\n",
        "        eigvals, eigvecs = cache[\"lap_eig\"]\n",
        "\n",
        "    return eigvals, eigvecs, base_level, cache\n",
        "\n",
        "def _get_positional_eigvecs(\n",
        "    matrix: torch.Tensor,\n",
        "    num_pos: int\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    matrix = matrix.to_dense()\n",
        "    eigvals, eigvecs = torch.linalg.eigh(matrix)\n",
        "\n",
        "    # Pad with non-sense eigenvectors if required\n",
        "    if num_pos > matrix.shape[0]:\n",
        "        temp_EigVal = torch.ones(num_pos - matrix.shape[0], dtype=torch.float64, device=device) + float(\"inf\")\n",
        "        temp_EigVec = torch.zeros((matrix.shape[0], num_pos - matrix.shape[0]), dtype=torch.float64, device=device)\n",
        "        eigvals = torch.cat([eigvals, temp_EigVal], dim=0)\n",
        "        eigvecs = torch.cat([eigvecs, temp_EigVec], dim=1)\n",
        "\n",
        "    # Sort and keep only the first `num_pos` elements\n",
        "    sort_idx = eigvals.argsort()\n",
        "    eigvals = eigvals[sort_idx]\n",
        "    eigvals = eigvals[:num_pos]\n",
        "    eigvecs = eigvecs[:, sort_idx]\n",
        "    eigvecs = eigvecs[:, :num_pos]\n",
        "\n",
        "    # Normalize the eigvecs\n",
        "    eigvecs = eigvecs / torch.maximum(torch.sqrt(torch.sum(eigvecs**2, dim=0, keepdim=True)), torch.tensor(1e-4, device=matrix.device, dtype=matrix.dtype))\n",
        "\n",
        "    return eigvals, eigvecs\n",
        "\n",
        "def normalize_matrix(\n",
        "    matrix: torch.Tensor,\n",
        "    degree_vector: torch.Tensor,\n",
        "    normalization: str = None\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    device = matrix.device\n",
        "\n",
        "    if degree_vector is not None:\n",
        "        degree_inv = degree_vector.pow(-0.5).unsqueeze(1).to_dense()  # Convert to dense for the assignment\n",
        "        degree_inv[torch.isinf(degree_inv)] = 0\n",
        "        degree_inv = degree_inv.to_sparse()\n",
        "\n",
        "    # Compute the normalized matrix\n",
        "    if (normalization is None) or (normalization.lower() == \"none\"):\n",
        "        pass\n",
        "    elif normalization.lower() == \"sym\":\n",
        "        matrix = degree_inv * matrix * degree_inv.T\n",
        "    elif normalization.lower() == \"inv\":\n",
        "        matrix = (degree_inv**2) * matrix\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f'`normalization` should be `None`, `\"None\"`, `\"sym\"` or `\"inv\"`, but `{normalization}` was provided'\n",
        "        )\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "XkEMQ2s5i57L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Position Levels\n",
        "from typing import Tuple, Union, List, Dict, Any, Optional\n",
        "import torch\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "\n",
        "def transfer_pos_level(\n",
        "    pe: torch.Tensor,\n",
        "    in_level: str,\n",
        "    out_level: str,\n",
        "    adj: Union[torch.Tensor, torch.sparse.FloatTensor],\n",
        "    num_nodes: int,\n",
        "    cache: Optional[Dict[str, Any]] = None,\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    pe = pe.to('cuda')\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = adj.to('cuda')\n",
        "\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    if in_level == \"node\":\n",
        "        if out_level == \"node\":\n",
        "            pass\n",
        "\n",
        "        elif out_level == \"edge\":\n",
        "            pe, cache = node_to_edge(pe, adj, cache)\n",
        "\n",
        "        elif out_level == \"nodepair\":\n",
        "            pe = node_to_nodepair(pe, num_nodes)\n",
        "\n",
        "        elif out_level == \"graph\":\n",
        "            raise NotImplementedError(\"Transfer function (node -> graph) not yet implemented.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown `pos_level`: {out_level}\")\n",
        "    elif in_level == \"edge\":\n",
        "        raise NotImplementedError(\"Transfer function (edge -> *) not yet implemented.\")\n",
        "    elif in_level == \"nodepair\":\n",
        "        if len(pe.shape) == 2:\n",
        "            pe = torch.unsqueeze(pe, -1)\n",
        "\n",
        "        if out_level == \"node\":\n",
        "            pe = nodepair_to_node(pe)\n",
        "\n",
        "        elif out_level == \"edge\":\n",
        "            pe, cache = nodepair_to_edge(pe, adj, cache)\n",
        "\n",
        "        elif out_level == \"nodepair\":\n",
        "            pass\n",
        "\n",
        "        elif out_level == \"graph\":\n",
        "            raise NotImplementedError(\"Transfer function (nodepair -> graph) not yet implemented.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown `pos_level`: {out_level}\")\n",
        "    elif in_level == \"graph\":\n",
        "        if out_level == \"node\":\n",
        "            pe = graph_to_node(pe, num_nodes, cache)\n",
        "\n",
        "        elif out_level in [\"edge\", \"nodepair\"]:\n",
        "            raise NotImplementedError(\"Transfer function (graph -> edge/nodepair) not yet implemented.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown `pos_level`: {out_level}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown `pos_level`: {in_level}\")\n",
        "\n",
        "    return pe\n",
        "\n",
        "def node_to_edge(\n",
        "    pe: torch.Tensor, adj: Union[torch.Tensor, torch.sparse.FloatTensor], cache: Optional[Dict[str, Any]] = None\n",
        ") -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "\n",
        "    pe = pe.to('cuda')\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = adj.to('cuda')\n",
        "\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    edge_index, _ = from_scipy_sparse_matrix(adj)\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "\n",
        "    pe_sum = pe[src] + pe[dst]\n",
        "    pe_abs_diff = torch.abs(pe[src] - pe[dst])\n",
        "\n",
        "    edge_pe = torch.cat((pe_sum, pe_abs_diff), dim=-1)\n",
        "\n",
        "    return edge_pe, cache\n",
        "\n",
        "def node_to_nodepair(pe: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "    pe = pe.to('cuda')\n",
        "    expanded_pe = torch.unsqueeze(pe, dim=1)\n",
        "    expanded_pe = expanded_pe.repeat(1, num_nodes, 1)\n",
        "\n",
        "    pe_sum = expanded_pe + expanded_pe.transpose(0, 1)\n",
        "    pe_abs_diff = torch.abs(expanded_pe - expanded_pe.transpose(0, 1))\n",
        "\n",
        "    nodepair_pe = torch.cat((pe_sum, pe_abs_diff), dim=-1)\n",
        "\n",
        "    return nodepair_pe\n",
        "\n",
        "def node_to_graph(pe: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "    raise NotImplementedError(\"Transfer function (node -> graph) not yet implemented.\")\n",
        "\n",
        "def edge_to_node(pe: torch.Tensor, adj: Union[torch.Tensor, torch.sparse.FloatTensor]) -> torch.Tensor:\n",
        "    raise NotImplementedError(\"Transfer function (edge -> node) not yet implemented.\")\n",
        "\n",
        "def edge_to_nodepair(\n",
        "    pe: torch.Tensor, adj: Union[torch.Tensor, torch.sparse.FloatTensor], num_nodes: int, cache: Optional[Dict[str, Any]] = None\n",
        ") -> torch.Tensor:\n",
        "    pe = pe.to('cuda')\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = adj.to('cuda')\n",
        "\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    num_feat = pe.shape[-1]\n",
        "\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = torch.sparse.FloatTensor(adj).to('cuda')\n",
        "    dst, src = adj.indices()[0], adj.indices()[1]\n",
        "\n",
        "    nodepair_pe = torch.zeros((num_nodes, num_nodes, num_feat), device='cuda')\n",
        "\n",
        "    for i in range(len(dst)):\n",
        "        nodepair_pe[dst[i], src[i], ...] = pe[i, ...]\n",
        "\n",
        "    return nodepair_pe, cache\n",
        "\n",
        "def edge_to_graph(pe: torch.Tensor) -> torch.Tensor:\n",
        "    raise NotImplementedError(\"Transfer function (edge -> graph) not yet implemented.\")\n",
        "\n",
        "def nodepair_to_node(pe: torch.Tensor, stats_list: List = [torch.min, torch.mean, torch.std]) -> torch.Tensor:\n",
        "    num_feat = pe.shape[-1]\n",
        "\n",
        "    node_pe_list = []\n",
        "\n",
        "    for stat in stats_list:\n",
        "        for i in range(num_feat):\n",
        "            node_pe_list.append(stat(pe[..., i], dim=0))\n",
        "            node_pe_list.append(stat(pe[..., i], dim=1))\n",
        "    node_pe = torch.stack(node_pe_list, dim=-1)\n",
        "\n",
        "    return node_pe\n",
        "\n",
        "def nodepair_to_edge(\n",
        "    pe: torch.Tensor, adj: Union[torch.Tensor, torch.sparse.FloatTensor], cache: Optional[Dict[str, Any]] = None\n",
        ") -> torch.Tensor:\n",
        "    pe = pe.to('cuda')\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = adj.to('cuda')\n",
        "\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    num_feat = pe.shape[-1]\n",
        "\n",
        "    if not isinstance(adj, torch.sparse.FloatTensor):\n",
        "        adj = torch.sparse.FloatTensor(adj).to('cuda')\n",
        "    dst, src = adj.indices()[0], adj.indices()[1]\n",
        "\n",
        "    edge_pe = torch.zeros((len(dst), num_feat), device='cuda')\n",
        "\n",
        "    for i in range(len(src)):\n",
        "        edge_pe[i, ...] = pe[dst[i], src[i]]\n",
        "\n",
        "    return edge_pe, cache\n",
        "\n",
        "def nodepair_to_graph(pe: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "    raise NotImplementedError(\"Transfer function (nodepair -> graph) not yet implemented.\")\n",
        "\n",
        "def graph_to_node(\n",
        "    pe: Union[torch.Tensor, List], num_nodes: int, cache: Optional[Dict[str, Any]] = None\n",
        ") -> torch.Tensor:\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    node_pe = None\n",
        "\n",
        "    # The key 'components' is only in cache if disconnected_comp == True when computing base pe\n",
        "    if \"components\" in cache:\n",
        "        if len(cache[\"components\"]) > 1:\n",
        "            node_pe = torch.zeros((num_nodes, len(pe)), device='cuda')\n",
        "            components = cache[\"components\"]\n",
        "\n",
        "            for i, component in enumerate(components):\n",
        "                comp = list(component)\n",
        "                node_pe[comp, :] = pe[i]\n",
        "\n",
        "    if node_pe is None:\n",
        "        node_pe = pe.repeat(num_nodes, 1)\n",
        "\n",
        "    return node_pe"
      ],
      "metadata": {
        "id": "OVac7uPrgOJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Positional Encodings\n",
        "from typing import Tuple, Union, Optional, Dict, Any, OrderedDict\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "from scipy.sparse import spmatrix\n",
        "from collections import OrderedDict as OderedDictClass\n",
        "\n",
        "# Define the device (GPU if available)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_all_positional_encodings(\n",
        "    adj: Union[torch.Tensor, spmatrix],\n",
        "    num_nodes: int,\n",
        "    pos_kwargs: Optional[Dict] = None,\n",
        ") -> Tuple[\"OrderedDict[str, torch.Tensor]\"]:\n",
        "\n",
        "    pos_kwargs = {} if pos_kwargs is None else pos_kwargs\n",
        "\n",
        "    pe_dict = OderedDictClass()\n",
        "    cache = {}\n",
        "\n",
        "    if len(pos_kwargs) > 0:\n",
        "        for pos_name, this_pos_kwargs in pos_kwargs[\"pos_types\"].items():\n",
        "            this_pos_kwargs = deepcopy(this_pos_kwargs)\n",
        "            pos_type = this_pos_kwargs.pop(\"pos_type\", None)\n",
        "            pos_level = this_pos_kwargs.pop(\"pos_level\", None)\n",
        "            this_pe, cache = graph_positional_encoder(\n",
        "                adj.clone(),\n",
        "                num_nodes,\n",
        "                pos_type=pos_type,\n",
        "                pos_level=pos_level,\n",
        "                pos_kwargs=this_pos_kwargs,\n",
        "                cache=cache,\n",
        "            )\n",
        "            if pos_level == \"node\":\n",
        "                pe_dict.update({f\"{pos_type}\": this_pe})\n",
        "            else:\n",
        "                pe_dict.update({f\"{pos_level}_{pos_type}\": this_pe})\n",
        "\n",
        "    return pe_dict\n",
        "\n",
        "def graph_positional_encoder(\n",
        "    adj: Union[torch.Tensor, spmatrix],\n",
        "    num_nodes: int,\n",
        "    pos_type: Optional[str] = None,\n",
        "    pos_level: Optional[str] = None,\n",
        "    pos_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    cache: Optional[Dict[str, Any]] = None,\n",
        ") -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "\n",
        "    pos_kwargs = deepcopy(pos_kwargs) if pos_kwargs else {}\n",
        "    cache = cache if cache else {}\n",
        "\n",
        "    pos_type2 = pos_kwargs.pop(\"pos_type\", None)\n",
        "    pos_level2 = pos_kwargs.pop(\"pos_level\", None)\n",
        "\n",
        "    # Conversion of tensors to device\n",
        "    adj = adj.to(device)\n",
        "\n",
        "    # Calculate positional encoding\n",
        "    if pos_type == \"laplacian_eigvec\":\n",
        "        _, pe, base_level, cache = compute_laplacian_pe(adj, cache=cache, **pos_kwargs)\n",
        "    elif pos_type == \"laplacian_eigval\":\n",
        "        pe, _, base_level, cache = compute_laplacian_pe(adj, cache=cache, **pos_kwargs)\n",
        "    elif pos_type == \"rw_return_probs\":\n",
        "        pe, base_level, cache = compute_rwse(\n",
        "            adj, num_nodes=num_nodes, cache=cache, pos_type=pos_type, **pos_kwargs\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown `pos_type`: {pos_type}\")\n",
        "\n",
        "    # Convert between different pos levels\n",
        "    if isinstance(pe, (list, tuple)):\n",
        "        pe = [transfer_pos_level(this_pe, base_level, pos_level, adj, num_nodes, cache) for this_pe in pe]\n",
        "    else:\n",
        "        pe = transfer_pos_level(pe, base_level, pos_level, adj, num_nodes, cache)\n",
        "\n",
        "    return pe, cache"
      ],
      "metadata": {
        "id": "sAgMHIRMldAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from io import BytesIO\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up GCS storage configurations\n",
        "SOURCE_BUCKET_NAME = 'pyg-molecular-sample'\n",
        "TARGET_BUCKET_NAME = 'pyg-molecular-positional-2'\n",
        "CLIENT = storage.Client()\n",
        "SOURCE_BUCKET = CLIENT.bucket(SOURCE_BUCKET_NAME)\n",
        "TARGET_BUCKET = CLIENT.bucket(TARGET_BUCKET_NAME)\n",
        "\n",
        "# Load PyG object from GCS\n",
        "def load_pyg_object_from_gcs(blob_name):\n",
        "    blob = SOURCE_BUCKET.blob(blob_name)\n",
        "    byte_stream = blob.download_as_bytes()\n",
        "    buffer = BytesIO(byte_stream)\n",
        "    content = torch.load(buffer)\n",
        "    data = content['graph_with_features']\n",
        "    return data.to(device)\n",
        "\n",
        "# Save PyG object with encodings to GCS\n",
        "def save_to_gcs(data, blob_name):\n",
        "    data = data.cpu()\n",
        "    buffer = BytesIO()\n",
        "    content = {'graph_with_features': data}\n",
        "    torch.save(content, buffer)\n",
        "    buffer.seek(0)\n",
        "    blob = TARGET_BUCKET.blob(blob_name)\n",
        "    blob.upload_from_file(buffer)\n",
        "\n",
        "# Compute the positional encodings and store them in the PyG object\n",
        "def compute_and_store_encodings(data):\n",
        "    adjacency_matrix = torch.zeros((data.num_nodes, data.num_nodes), device=device)\n",
        "    for (src, dst) in data.edge_index.T:\n",
        "        src, dst = int(src), int(dst)  # Convert to int for indexing\n",
        "        adjacency_matrix[src][dst] = 1\n",
        "        adjacency_matrix[dst][src] = 1\n",
        "\n",
        "    pos_kwargs = {\n",
        "        \"pos_types\": {\n",
        "            \"lap_eigvec\": {\n",
        "                \"pos_level\": \"node\",\n",
        "                \"pos_type\": \"laplacian_eigvec\",\n",
        "                \"num_pos\": 8,\n",
        "                \"normalization\": \"none\",\n",
        "            },\n",
        "            \"lap_eigval\": {\n",
        "                \"pos_level\": \"node\",\n",
        "                \"pos_type\": \"laplacian_eigval\",\n",
        "                \"num_pos\": 8,\n",
        "                \"normalization\": \"none\",\n",
        "            },\n",
        "            \"rw_return_probs\": {\n",
        "                \"pos_type\": \"rw_return_probs\",\n",
        "                \"pos_level\": \"node\",\n",
        "                \"ksteps\": [4, 8]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = get_all_positional_encodings(adjacency_matrix, data.num_nodes, pos_kwargs=pos_kwargs)\n",
        "\n",
        "    # Directly attach the tensor attributes to the PyG Data object\n",
        "    for key, encoding in results.items():\n",
        "        setattr(data, key, encoding.to(device))\n",
        "\n",
        "    return data\n",
        "\n",
        "for blob in SOURCE_BUCKET.list_blobs():\n",
        "    data = load_pyg_object_from_gcs(blob.name)\n",
        "    updated_data = compute_and_store_encodings(data)\n",
        "    save_to_gcs(updated_data, blob.name)\n",
        "\n",
        "    # Explicitly delete and free GPU memory\n",
        "    del data\n",
        "    del updated_data\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "9ArQjIJCpyE0",
        "outputId": "afd563b3-2360-4ff2-f8a0-380e5521e412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a43f97f60392>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pyg_object_from_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mupdated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_and_store_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0msave_to_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Explicitly delete and free GPU memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a43f97f60392>\u001b[0m in \u001b[0;36msave_to_gcs\u001b[0;34m(data, blob_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTARGET_BUCKET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Compute the positional encodings and store them in the PyG object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mupload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2540\u001b[0;31m             created_json = self._do_upload(\n\u001b[0m\u001b[1;32m   2541\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m                 \u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             )\n\u001b[1;32m   2370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m             response = self._do_resumable_upload(\n\u001b[0m\u001b[1;32m   2372\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_resumable_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit_next_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataCorruption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0;31m# Attempt to delete the corrupted object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/resumable_media/requests/upload.py\u001b[0m in \u001b[0;36mtransmit_next_chunk\u001b[0;34m(self, transport, timeout)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         return _request_helpers.wait_and_retry(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mretriable_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/resumable_media/requests/_request_helpers.py\u001b[0m in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_CONNECTION_ERROR_CLASSES\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# Fall through to retry, if there are retries left.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/resumable_media/requests/upload.py\u001b[0m in \u001b[0;36mretriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# Wrap the request business logic in a function to be retried.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretriable_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             result = transport.request(\n\u001b[0m\u001b[1;32m    508\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    715\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}